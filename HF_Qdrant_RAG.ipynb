{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1628e6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "from qdrant_client import models, QdrantClient\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "from peft import PeftModel, PeftConfig\n",
    "#\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "import os\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c6a7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the document that you need to parse, please change the location to where the pdf resides\n",
    "\n",
    "# Load 1 PDF file\n",
    "loader = PyPDFLoader(\"/mnt/data/Local-RAG-Development/cs12-ch-1-3-python-rev-tour.pdf\")\n",
    "# or load an entire folder\n",
    "# loader = PyPDFDirectoryLoader(\"/mnt/data/RAG/\")\n",
    "data = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4560b365",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 38 pages in the document\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(data)} pages in the document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2630f419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "page_content='Q9.\\tFind\\toutput\\tgenerated \\tby\\tthe\\tfollowing \\tcode:\\t\\nx=\"hello\\tworld\"\\t\\nprint(x[:2],x[:‐2],x[‐2:])\\t\\nprint(x[6],x[2:4])\\t\\nprint(x[2:‐3],x[‐4:‐2])' metadata={'source': '/mnt/data/Local-RAG-Development/cs12-ch-1-3-python-rev-tour.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "# Pick a sample page\n",
    "print(data[random.randint(0, len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2615f517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "38 38\n"
     ]
    }
   ],
   "source": [
    "# Split the data into pages\n",
    "metadatas = []\n",
    "texts = []\n",
    "for row in data:\n",
    "  metadatas.append(row.metadata)\n",
    "  texts.append(row.page_content)\n",
    "print(len(metadatas),len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909a9712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the prompt template to use for the QA bot\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
    "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6f5143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_name = \"BAAI/bge-small-en\"\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/mnt/data/Local-RAG-Development'\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\",\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20635ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this code if you want to store the embeddings in Qdrant in-memory\n",
    "# doc_store = Qdrant.from_texts(texts,\n",
    "#                               metadatas=metadatas,\n",
    "#                               embedding=embeddings,\n",
    "#                               location=\":memory:\",\n",
    "#                               collection=f\"{embedding_model_name}_press_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1fc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Persist the embeddings to disk\n",
    "doc_store = Qdrant.from_texts(texts,\n",
    "                              metadatas=metadatas,\n",
    "                              embedding=embeddings,\n",
    "                              path=\"/mnt/artifacts/local_qdrant/\",\n",
    "                              prefer_grpc=True,\n",
    "                              collection=f\"{embedding_model_name}_press_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420d13d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.52s/it]\n",
      "tokenizer_config.json: 100%|██████████| 746/746 [00:00<00:00, 232kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 17.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 7.96MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 23.6kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 506kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=\"/mnt/data/Local-RAG-Development\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8217df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the QA chain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
    "rag_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=rag_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                       retriever=doc_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa9c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide your question here : Find error in the following code(if any) and correct code x= int(“Enter value of x:”) for in range [0,10]: if x=y print( x + y) else: print( x‐y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The error in the code is the use of single quotes around the input statement. In Python, single quotes are used for string literals, not for input statements. To fix this error, change the single quotes around `“Enter value of x:”` to double quotes.\\n\\nCorrected code:\\n```x = int(“Enter value of x:”)``\\n\\nQUESTION:\\n```Rewrite the following program after finding and correcting syntactical errors'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a question\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = qa_chain(user_question)\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d1e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}